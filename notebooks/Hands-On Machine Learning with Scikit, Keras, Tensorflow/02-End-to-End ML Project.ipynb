{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43e355e-fd8f-4b12-bc4e-459977b24bd5",
   "metadata": {},
   "source": [
    "# Chapter 2: End-to-End Machine Learning Project\n",
    "\n",
    "This chapter runs through an example project. Specifically, this chapter will use data about California housing prices. The main steps that they walk through are:\n",
    "\n",
    "1) Look at the big picture.\n",
    "2) Get the data.\n",
    "3) Explore and visualize the data to gain insight.\n",
    "4) Prepare the data for ML algorithms\n",
    "5) Select a model and train it.\n",
    "6) Fine tune your model.\n",
    "7) Present your solution.\n",
    "8) Launch, monitor, and maintain your system. \n",
    "\n",
    "## Look at the Big Picture\n",
    "\n",
    "In this chapter, we use data from the California census. Our task is to use this data to build a model of housing prices in the state. The data includes metrics such as population, median income, and median housing price for each block group in California (\"districts\").\n",
    "\n",
    "The first thing we will want to do, is look at Appendix A - our machine learning project checklist. This can be seen in the `Appendix A - Machine Learning Checklist.ipynb` notebook.\n",
    "\n",
    "#### Frame the Problem\n",
    "\n",
    "The first question is to ask the boss *what exactly the business objective is*. Building a model is probably not the end goal. How does the company expect to use and benefit from this model? \n",
    "\n",
    "The next question is to ask the boss *what current solutions look like (if any)*. This can give a reference for performance, as well as insights on how to solve the problem. \n",
    "\n",
    "Once these questions are asked and answered, we can figure out the following:\n",
    "* Are we looking for a supervised, unsupervised, or other type of system?\n",
    "* Is it a classification task or regression? What's the response?\n",
    "\n",
    "In the example in this chapter, we will be using a supervised regression system, as our target is median housing price - which is a continous variable. We could bin these values and make it a classification task, but we will treat it as regression.\n",
    "\n",
    "#### Select a Performance Measure\n",
    "\n",
    "By selecting a performance measure, we can determine how well our system generates predictions. \n",
    "\n",
    "For regression, common metrics are:\n",
    "* Mean Absolute Error (MAE)\n",
    "* Mean Squared Error (MSE)\n",
    "* Root Mean Squared Error (RMSE)\n",
    "* R-Squared (Coefficient of Determination)\n",
    "* Adjusted R-Squared\n",
    "* Mean Absolute Percentage Error (MAPE)\n",
    "* AIC/BIC\n",
    "\n",
    "For classification, common metrics are:\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1 Score\n",
    "* Specificity\n",
    "* Sensitivity\n",
    "* ROC/AUC\n",
    "\n",
    "A more indepth explanation for these can be found in the `docs/performance_measures.docx` file. In this problem, we elect to use RMSE.\n",
    "\n",
    "Metrics like RMSE and MAE are both ways to measure the distance between two vectors (the vector of predictions, and the vector of actual targets). Computing the RMSE corresponds to the Euclidean norm (l2 norm). The MAE is referred to as the Manhattan norm (l1 norm).\n",
    "\n",
    "The higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. When outliers are exponentially rare, the RMSE performs very well and is generally preferred.\n",
    "\n",
    "#### Check the Assumptions\n",
    "\n",
    "Before fully diving in and writing code, it's a good practice to verify the assumptions that have been made so far. This helps catch serious issues early on. \n",
    "\n",
    "## Get the Data\n",
    "\n",
    "Usually we will load in the data from a database - which involves a much more complicated process. In this example, we will simply load in the data from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88858366-96d3-4705-a4b9-553e68480ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "\n",
    "# download data\n",
    "data_path = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "data_save_path = \"../../data/Hands-On Machine Learning with Scikit, Keras, Tensorflow/housing_data.tgz\"\n",
    "urllib.request.urlretrieve(data_path, data_save_path)\n",
    "\n",
    "# extract data\n",
    "with tarfile.open(data_save_path) as housing_tarball_file:\n",
    "    housing_tarball_file.extractall(path = \"../../data/Hands-On Machine Learning with Scikit, Keras, Tensorflow\")\n",
    "\n",
    "# load data\n",
    "housing_data = pd.read_csv(\"../../data/Hands-On Machine Learning with Scikit, Keras, Tensorflow/housing/housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89223ed-ec24-42e7-a8a9-66f9771a8acc",
   "metadata": {},
   "source": [
    "## Explore + Visualize Data to Gain Insights\n",
    "\n",
    "Each row represents one district. There are 10 total features. They are:\n",
    "* longitude\n",
    "* lattitude\n",
    "* housing_median_age\n",
    "* total_rooms\n",
    "* total_bedrooms\n",
    "* population\n",
    "* households\n",
    "* median_income\n",
    "* median_house_value\n",
    "* ocean_proximity\n",
    "\n",
    "There ar 20,640 instances in the dataset. All attributes are numerical except for ocean_proximity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e130f6-81ba-42de-8956-549fcc4dc94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db276496-a003-46b4-b63b-dd1341470a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017fb4c-b183-4e0e-837b-34b9eb816ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727c9e8-5f85-43ca-a747-f86ce61f6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "housing_data.hist(bins = 50, figsize = (12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f9be9-9744-433c-b5b8-249255650494",
   "metadata": {},
   "source": [
    "The book recommends creating a test set at this stage. They recommend that we avoid ***snooping bias*** - avoid recognizing patterns in the test set that could lead us down the path of overfitting.\n",
    "\n",
    "In this example, the book gives the example that the median income feature income could be very important in the prediction process. We want to make sure that our train and test set have a representative sample and include a sufficient amount of each strata of median income. The book makes median income into an ordinal variable, and then performs a stratified split based on this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3a634-1b5c-451f-b0a0-9339d1d9b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "# random split\n",
    "train_data, test_data = train_test_split(housing_data, test_size = 0.2, random_state = 1230)\n",
    "\n",
    "# make median income into ordinal variable\n",
    "housing_data[\"income_category\"] = pd.cut(\n",
    "    housing_data[\"median_income\"], \n",
    "    bins = [0, 1.5, 3, 4.5, 6, np.inf], \n",
    "    labels = [1, 2, 3, 4, 5]\n",
    ")\n",
    "\n",
    "# stratified split\n",
    "train_data, test_data = train_test_split(housing_data, test_size = 0.2, stratify = housing_data[\"income_category\"], random_state = 1230)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4f0e1-87ec-4000-9822-72afd278e194",
   "metadata": {},
   "source": [
    "The book then:\n",
    "* Plots and visualizes features\n",
    "* Analyzes correlations - correlations with panda + scatter_matrix()\n",
    "* Plays with some of the features and attempts to make new features that are more insightful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3db4e0-14ef-4968-b506-a4112bf62637",
   "metadata": {},
   "source": [
    "## Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "The book recommends writing functions for each of the steps so that we can re-use our code (on new projects, a live deployment, and on the test set). \n",
    "\n",
    "***Clean the Data***\n",
    "\n",
    "Most ML algorithms cannot work with missing features. The book shows how to impute with the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e204d5-0bec-455a-ac80-8cdfc2eec536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# instantiate imputer\n",
    "imputer = SimpleImputer(strategy = \"median\")\n",
    "\n",
    "# fit (only on numerical)\n",
    "housing_numerical_features = housing_data.select_dtypes(include = [np.number])\n",
    "imputer.fit(housing_numerical_features)\n",
    "X = imputer.transform(housing_numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fbf1b6-e724-4ffc-ac4b-bb14d1a8b550",
   "metadata": {},
   "source": [
    "***Scikit-Learn Design***\n",
    "\n",
    "All objects in Scikit-Learn share a consistent and similar interface.\n",
    "\n",
    "1) ***Estimators*** - Any object that can estimate some parameters based on a dataset is called an *estimator*. The estimation itself is performed by the *.fit()* method. And it takes the data as an argument.\n",
    "2) ***Transformers*** - Some estimators can transform a dataset. The transformation is performed by the *.transform()* method. It returns the transformed dataset. All transformers have a convienence function called *.fit_transform()* which calls both the *.fit()* and *.transform()* methods.\n",
    "3) ***Predictors*** - Some estimators, given a dataset, are capable of making predictions. These have the *.predict()* method.\n",
    "\n",
    "Note: Scikit-Learn transformers output NumPy arrays, even when they are fed Pandas DataFrames. \n",
    "\n",
    "***One-Hot Encoding***\n",
    "\n",
    "The book transforms a categorical variable, ocean proximity, with One-Hot Encoding. One-Hot Encoding makes a unique column for each category in a feature. It will return a binary value, 1 if that category exists in that observation and 0 if not.\n",
    "\n",
    "The result of this is a SciPy *sparse matrix*, which is a matrix that contains mostly zeros. Be cautious of performing One-Hot Encoding when there are many categories in a feature. A sparse matrix can be transformed to a regular matrix by calling *.toarray()* (alternatively, you can set *sparse=False*).\n",
    "\n",
    "Pandas also has a function called *.get_dummies()* which converts each categorical feature into a one-hot representation. It is recommended to use the Scikit-Learn method as it raises excpetions if the same transformation is attempted to be applied on different features. \n",
    "\n",
    "***Note***: When you fit any Scikit-Learn estimator using a DataFrame, it stores thge column names in the *feature_names_in_* attribute. It also provides a *get_feature_names_out()* method that can be used to build a DataFrame around the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9181ec-e7aa-4d8b-a774-5e2d39a851d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "original_encoder = OneHotEncoder()\n",
    "housing_cat_encoded = original_encoder.fit_transform(housing_data.loc[:, [\"ocean_proximity\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee77dbf-3a7c-40ab-a482-3a4f0df3c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_encoder.feature_names_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bcdfc6-414d-4f46-9e7c-c053bbba43ab",
   "metadata": {},
   "source": [
    "***Feature Scaling and Transformation***\n",
    "\n",
    "ML algorithms often don't perform very well when the numerical attributes have different scales. There are two common ways to get all attributes to have the same scale:\n",
    "* Min-Max Scaling\n",
    "* Standardization\n",
    "\n",
    "Min-Max Scaling is the simplest: for each attribute the values are shifted and rescaled so that they end up ranging from 0 to 1. This is performed by subtracting the min value and dividing by the difference between the min and the max. *MinMaxScaler* does this. It has a *feature_range* hyperparameter which lets you change the range if you'd like (i.e some NN's prefer values between -1, 1).\n",
    "\n",
    "Standardization subtracts the mean value, then divides by the standard deviation. So this method does not restrict values to a specific range. Standardization is much less affected by outliers. We can perform this with *StandardScaler*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8896d2d-b78e-4418-bd4a-273ae2e417ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# min-max scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "housing_numerical_min_max_scaled = min_max_scaler.fit_transform(housing_numerical_features)\n",
    "\n",
    "\n",
    "# standardization\n",
    "std_scaler = StandardScaler()\n",
    "housing_numerical_std_scaled = std_scaler.fit_transform(housing_numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef6a88-23cc-47d4-8baa-3ffcbffd0de3",
   "metadata": {},
   "source": [
    "When a features distribution has a *heavy tail*, both methods of transformations will squash most values in a small range. Most ML models do not like this. So before scaling the feautre, we will want to get rid of the heavy tail.\n",
    "\n",
    "When we have a heavy tail to the right, we may want to take the *square root*. For features with a heavy tail to the left, we may want to take the *logarithm*. \n",
    "\n",
    "Another approach to handling heavy-tailed features consists in ***bucketizing*** the feature. This means chopping it's distribution into roughly equal-size buckets, and replacing each feature value with the index of the bucket it belongs to. For example, we could replace each value with it's percentile. This results in a uniform distribution, so there's no need for further scaling. \n",
    "\n",
    "When a feature has a multimodal distribution, it can be helpful to bucketize it. For this, it may be beneficial to treat the bucket ID's as categories, rather than as numerical values. This means we can one-hot encoded the bucket indixes.\n",
    "\n",
    "Another approach for handling this is to add a feature for each of the modes, representing the similarity betwen the feature and that particular mode. This similarity measure is typical computed using a ***radial basis function (RBF)*** - which is any function that depends only on the distance bnetween the input value and a fixed point. The most commonly used RBF is the ***Gaussian RBF***, whose output value decays exponentially as the input value moves away from the fixed point. We can use this with the *rbf_kernel* function. There is a hyperparameter that controls how quickly the similarity measure decays as x moves away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58277392-9c05-4bbc-81cd-2bdd67ba6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "# Gaussian RBF\n",
    "age_simil_35 = rbf_kernel(housing_data[[\"housing_median_age\"]], [[35]], gamma = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15283844-f3c7-4cfd-b19d-3ecb382bf8a2",
   "metadata": {},
   "source": [
    "So far, we've looked at how to transform input features. However, we need to be concious that the target tag may need to be tranformed as well. ***If you make a transformation to the target, you must apply the inverse transformation to get the proper values when predicting***. Thankfully, transformers have an *.inverse_transform()* method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf13b2-f92f-4cbb-83cb-6c237bd63a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# target\n",
    "housing_labels = housing_data.loc[:, \"median_house_value\"]\n",
    "\n",
    "# scale target\n",
    "target_scaler = StandardScaler()\n",
    "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n",
    "\n",
    "# fit linear reg\n",
    "model = LinearRegression()\n",
    "model.fit(housing_data[[\"median_income\"]], scaled_labels)\n",
    "\n",
    "# predictions + scale back\n",
    "# NOTE: this code will not run unless you define variable `some_new_data`\n",
    "scaled_predictions = model.predict(some_new_data)\n",
    "prediction = target_scaler.inverse_transform(scaled_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ed43d-5a10-4a18-b348-7560c00c6417",
   "metadata": {},
   "source": [
    "### Custom Scikit-Learn Transformers\n",
    "\n",
    "Custom transformers can be useful to combine features (see below, which calculates the ratio beween two variables).\n",
    "\n",
    "Wa can also add a parameter *inverse_func*, which will can be used to calculate the inverse (see below in the second example, which calculates the log)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84eee9-abd7-4a0a-b7fc-c133e9610441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# combine features with FunctionTransformer\n",
    "ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\n",
    "ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))\n",
    "\n",
    "# take log\n",
    "log_transformer = FunctionTransformer(np.log, inverse_func = np.exp)\n",
    "log_pop = log_transformer.transform(housing_data[[\"population\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4cdf7-df02-4424-85fd-401953549562",
   "metadata": {},
   "source": [
    "### Transformation Pipelines\n",
    "\n",
    "As we begin to increase the number of transformations, it becomes important to chain and organize them. Scikit-Learn offers the Pipeline class to help with such sequences.\n",
    "\n",
    "This takes in tuples, which represents the transformation steps. The name can be anything (as long as it does not contain *dunders*). The estimators must all be trasformers (aka they have the fit_transform()) method.\n",
    "\n",
    "When we call *fit()* on the pipeline, it calls *fit_transform()* sequentially on all the transformers, passing the output of each call as the parameter to the next call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55df2fd-2405-4e43-8c3d-b8e85b69797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# tranform numerical values\n",
    "numerical_pipeline = Pipeline(\n",
    "    [(\"impute\", SimpleImputer(strategy = \"mean\")),\n",
    "     (\"standardize\", StandardScaler()),\n",
    "    ])\n",
    "housing_numerical_prepared = numerical_pipeline.fit_transform(housing_numerical_features)\n",
    "\n",
    "# cast to dataframe\n",
    "housing_numerical_prepared_data = pd.DataFrame(\n",
    "    housing_numerical_prepared,\n",
    "    columns = numerical_pipeline.get_feature_names_out(),\n",
    "    index = housing_numerical_features.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4f8b6-e813-43fc-8bf8-6a311c87105f",
   "metadata": {},
   "source": [
    "Up to this point, we've handle the numerical and categorical attributes seperately. It would be much more convienent to have a single transformer capable of handling all columns. We can use *ColumnTransformer* for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c745d4-3176-4872-91b1-525b1f6f8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# define attributes\n",
    "numerical_attributes = [\"longitude\", \"latitude\", \"housing_median_age\"]\n",
    "categorical_attributes = [\"ocean_proximity\"]\n",
    "\n",
    "# make pipeline\n",
    "categorical_pipeline = Pipeline(\n",
    "    [(\"simple_impute\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "     (\"one_hot\", OneHotEncoder(handle_unknown = \"ignore\")),\n",
    "    ])\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", numerical_pipeline, numerical_attributes),\n",
    "    (\"cat\", categorical_pipeline, categorical_attributes)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410adaa-4aa5-44ef-ae77-ecf33398176b",
   "metadata": {},
   "source": [
    "Since listing out all the names is not very convenient, Scikit-Learn allows you to specify them easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddcb0dc-cecb-4b7e-b4f3-c5dbaad0f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "# define transformer\n",
    "preprocessing = make_column_transformer(\n",
    "    (numerical_pipeline, make_column_selector(dtype_include = np.number)),\n",
    "    (categorical_pipeline, make_column_selector(dtype_include = object))\n",
    ")\n",
    "\n",
    "# apply transformer\n",
    "housing_prepared = preprocessing.fit_transform(housing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c57733-f071-463a-89e6-9f64280e9531",
   "metadata": {},
   "source": [
    "## Select and Train a Model\n",
    "\n",
    "***Ensemble Methods***\n",
    "\n",
    "One way to fine tune your system is to try to combine the models that perform best. The group (\"ensemble\") will often perform better than the best individual model - just like random forests often perform better than the individual decision trees they rely on. This is especially the case if the individual models make very different types of errors.\n",
    "\n",
    "For example, we can train and fine-tune a k-nearest neighbors algorithm, then create an ensemble model that just predicts the mean of the random fores prediction and the models prediction.\n",
    "\n",
    "***Analyzing the Best Models and Their Errors***\n",
    "\n",
    "You can often gain good insights on the problem by insepcting the best models. For example, the Random Froest can indicate the relative importance of each attribute for making accurate predictions with *.feature_importances_*.\n",
    "\n",
    "We will also want to look at the specific errors our system makes. Then try to understand why it makes them , and what could fix the problem. Could we add extra features, get rid of uninformative ones, or clean up outliers?\n",
    "\n",
    "## Launch, Monitor, and Maintain Your System\n",
    "\n",
    "It is very important to write monitoring code to check our system's live performance at regular intervals, and trigger alerts when it drops. \n",
    "\n",
    "If the data keeps evolving, we will need to update our datasets and retrain our model's regularly. \n",
    "\n",
    "We will also want to write scripts to check our systems input data quality. \n",
    "\n",
    "We will want to make sure we version our models and datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
