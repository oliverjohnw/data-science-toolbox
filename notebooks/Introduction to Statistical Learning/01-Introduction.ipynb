{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf5d4cd-ea4a-4223-ba2c-27ab7829f5f2",
   "metadata": {},
   "source": [
    "# Chapter 1 - Introduction\n",
    "\n",
    "### An Overview of Statistical Learning\n",
    "\n",
    "Statistical learning refers to a vast set of tools for understanding data. These tools can be *supervised* or *unsupervised*. Broadly speaking, supervised techiniques involve building a statistical model for predicting an output based on one or more inputs. Unsupervised techniques rely on input, but no supervising outputs; which helps us learn about relationships and structures that lie within the data. \n",
    "\n",
    "The book provides 3 examples:\n",
    "1) Predicting wage (continuous, regression) based on age, year, and education.\n",
    "2) Predicting whether the stock market will increase or decrease (categorical, classification) based the past 5 days percentage changes in the index.\n",
    "3) Representing gene expression data in two-dimensional space (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7cad0-4dc4-416b-bb6f-61f1c090e1dc",
   "metadata": {},
   "source": [
    "### A Brief History of Statistical Learning\n",
    "\n",
    "Though the term is new, many of the concepts were developed long ago. **Least squares** was developed in the 1800's, and is the earliest form of what is now known as **linear regression**.  **Linear discriminant analysis** was proposed in 1936, and **logistic regression** in the 1940's. By the 1970's **generalized linear model** were developed. By the 1970's, many more techniques were available, however, they were almost execlusively linear methods (because fitting non-linear relationships was computationally difficult, at the time).\n",
    "\n",
    "By the 1980's, computing technology had finally improved sufficiently that non-linear methods were attainable. In the mid 1980's, **classification and regression trees** were developed, followed shortly by **generative additive models**. **Neural networks** gained popularity in the 1980's, and **support vector machines** arose in the 1990's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eedebf9-3b43-499b-9911-bae7e9013ba7",
   "metadata": {},
   "source": [
    "### This Book\n",
    "\n",
    "This book is the follow up to *Elements of Statistical Learning (2001)*, which is more technical and in-depth. This book is based on 4 principles:\n",
    "1) Many statistical learning methods are relevant and useful in a wide range of academic and non-academic disciplines, beyond just the staitstical sciences.\n",
    "2) Statistical learning should not be viewed as a series of black boxes. No single approach will perform well in all possible applications. Without understanding all of the cogs inside the box, or the interaction between those cogs, it is impossible to select the best box.\n",
    "3) While it is important to know what job is performed by each cog, it is not necessary to have the skills to contrsuct the machine inside the box.\n",
    "4) This book presumes that the reader is interested in applying statistical learning methods to real-world problems.\n",
    "\n",
    "***BOOK WEBSITE***: www.statlearning.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
